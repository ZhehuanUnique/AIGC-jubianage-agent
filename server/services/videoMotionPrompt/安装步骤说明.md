# Ollama 安装和配置步骤说明

## 📍 命令执行位置说明

### 步骤 1: 安装 Ollama
**执行位置：** 浏览器下载，然后运行安装程序

**方式一：直接下载安装（推荐）**
1. 访问 Ollama 官网：https://ollama.com/download
2. 下载 Windows 版本的安装程序（.exe 文件）
3. 双击运行安装程序，按照提示完成安装

**方式二：使用 winget（如果系统支持）**
```bash
winget install Ollama.Ollama
```
**注意：** 如果提示 `winget` 命令不存在，请使用方式一直接下载安装。

**说明：** 安装完成后，Ollama 会作为系统服务运行。可以在任意位置使用 `ollama` 命令。

---

### 步骤 2: 下载模型
**执行位置：** 任意位置（PowerShell 或命令提示符）

```bash
ollama pull qwen2.5:7b
```

**说明：** Ollama 是全局命令，可以在任何文件夹下执行。模型会下载到 Ollama 的默认目录（通常是 `C:\Users\你的用户名\.ollama\models`）。

**验证下载：**
```bash
ollama list
```

---

### 步骤 3: 配置环境变量
**执行位置：** 需要手动编辑文件

**文件路径：** `server/.env`（项目根目录下的 `server` 文件夹）

**操作方式：**
1. 打开 `server/.env` 文件（如果不存在，复制 `server/env.example` 并重命名为 `.env`）
2. 在文件末尾添加以下内容：

```env
# Ollama 配置
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# RAG 配置（可选）
RAG_ENABLED=true
RAG_VECTOR_DB_PATH=./data/rag_vectors
```

**说明：** 这是配置文件，需要手动编辑，不是命令行执行。

---

### 步骤 4: 启动服务并测试
**执行位置：** 在 `server` 文件夹下

**步骤 4.1: 启动后端服务**
```bash
# 1. 进入 server 文件夹
cd server

# 2. 启动服务
npm start
```

**步骤 4.2: 测试 Ollama 服务（新开一个终端窗口）**
**执行位置：** 任意位置

```bash
# 检查服务状态
curl http://localhost:3002/api/ollama/health
```

或者使用浏览器访问：
```
http://localhost:3002/api/ollama/health
```

---

## 📂 完整操作流程

### 方式一：使用项目根目录

```bash
# 1. 在项目根目录（AIGC-jubianage-agent）下
# 安装 Ollama（只需执行一次）
# 方式一：访问 https://ollama.com/download 下载安装
# 方式二：如果系统支持 winget，可以使用：winget install Ollama.Ollama

# 2. 下载模型（只需执行一次）
ollama pull qwen2.5:7b

# 3. 编辑 server/.env 文件（手动操作，添加配置）

# 4. 启动服务
cd server
npm start

# 5. 在另一个终端测试（可以在任意位置）
curl http://localhost:3002/api/ollama/health
```

### 方式二：直接在 server 文件夹下

```bash
# 1. 进入 server 文件夹
cd server

# 2. 安装 Ollama（只需执行一次）
# 访问 https://ollama.com/download 下载安装
# 或使用：winget install Ollama.Ollama（如果系统支持）

# 3. 下载模型（只需执行一次，可以在任意位置）
ollama pull qwen2.5:7b

# 4. 编辑 .env 文件（手动操作，添加配置）

# 5. 启动服务
npm start

# 6. 在另一个终端测试（可以在任意位置）
curl http://localhost:3002/api/ollama/health
```

---

## ⚠️ 重要提示

1. **安装和下载模型**：可以在任意位置执行，只需执行一次
2. **配置环境变量**：需要手动编辑 `server/.env` 文件
3. **启动服务**：必须在 `server` 文件夹下执行 `npm start`
4. **测试命令**：可以在任意位置执行

---

## 🔍 验证安装是否成功

### 检查 Ollama 是否安装成功
```bash
ollama list
```
应该能看到 `qwen2.5:7b` 模型

### 检查后端服务是否启动
访问：http://localhost:3002/health

### 检查 Ollama 集成是否正常
访问：http://localhost:3002/api/ollama/health

应该返回：
```json
{
  "success": true,
  "data": {
    "healthy": true,
    "model": "qwen2.5:7b",
    "baseUrl": "http://localhost:11434",
    "ragEnabled": true
  }
}
```

