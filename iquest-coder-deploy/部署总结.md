# 🎉 IQuest-Coder-V1-40B 部署方案总结

## 📦 已创建的文件

```
iquest-coder-deploy/
├── README.md                    # 完整部署文档
├── QUICK_START.md              # 快速开始指南（3分钟部署）
├── 集成到现有项目.md            # 集成到你的 AIGC 项目
├── 部署总结.md                  # 本文件
├── Dockerfile                   # Docker 镜像构建文件
├── docker-compose.yml          # Docker Compose 配置
├── start_vllm.sh               # vLLM 启动脚本
├── install.sh                  # 环境安装脚本（直接部署）
├── start_service.sh            # 服务启动脚本（直接部署）
├── stop_service.sh             # 服务停止脚本（直接部署）
├── test_api.py                 # API 测试脚本
├── client_example.py           # 客户端使用示例
└── .env.example                # 环境变量示例
```

## 🚀 部署方式对比

### 方式 1: Docker 部署（推荐）

**优点**：
- ✅ 一键部署，最简单
- ✅ 环境隔离，不污染系统
- ✅ 易于迁移和扩展
- ✅ 自动重启和健康检查

**缺点**：
- ❌ 需要 Docker 环境
- ❌ 首次构建镜像需要时间

**适用场景**：
- 生产环境部署
- 需要快速部署和迁移
- 团队协作开发

**部署命令**：
```bash
docker-compose up -d
```

### 方式 2: 直接部署

**优点**：
- ✅ 不需要 Docker
- ✅ 更灵活的配置
- ✅ 便于调试

**缺点**：
- ❌ 需要手动安装依赖
- ❌ 环境配置复杂
- ❌ 可能与系统环境冲突

**适用场景**：
- 开发测试环境
- 需要深度定制
- 不支持 Docker 的环境

**部署命令**：
```bash
sudo bash install.sh
bash start_service.sh
```

## 🎯 关键配置说明

### 1. 显存优化（针对 RTX 5090 24GB）

```bash
# start_vllm.sh 中的关键参数
GPU_MEMORY_UTILIZATION=0.90  # 使用 90% 显存
MAX_MODEL_LEN=32768          # 最大上下文长度
QUANTIZATION="bitsandbytes"  # 4-bit 量化
```

**如果显存不足**，调整为：
```bash
GPU_MEMORY_UTILIZATION=0.85
MAX_MODEL_LEN=16384
```

### 2. 性能优化

```bash
ENABLE_CHUNKED_PREFILL="true"    # 启用分块预填充
MAX_NUM_BATCHED_TOKENS=8192      # 批处理 token 数
MAX_NUM_SEQS=256                 # 最大并发序列数
```

### 3. 采样参数（官方推荐）

```bash
DEFAULT_TEMPERATURE=0.6  # 温度
DEFAULT_TOP_P=0.85       # Top-P 采样
DEFAULT_TOP_K=20         # Top-K 采样
```

## 📊 性能预估

### 硬件配置
- **GPU**: RTX 5090 (24GB VRAM)
- **量化**: 4-bit (bitsandbytes)
- **模型**: IQuest-Coder-V1-40B-Loop-Instruct

### 预期性能
- **首次加载时间**: 10-20 分钟（下载模型）
- **推理速度**: 约 20-30 tokens/秒
- **最大上下文**: 32K tokens（可调整）
- **并发能力**: 支持多用户同时请求

### 显存占用
- **模型加载**: 约 18-20GB
- **推理时**: 约 20-22GB
- **剩余显存**: 约 2-4GB（用于缓存）

## 🌐 API 接口说明

### 基础信息
- **地址**: `http://你的服务器IP:8000`
- **协议**: OpenAI 兼容 API
- **认证**: 不需要（vLLM 默认）

### 主要端点
```
GET  /health                    # 健康检查
GET  /v1/models                 # 模型列表
POST /v1/chat/completions       # 聊天补全
POST /v1/completions            # 文本补全
```

### 请求示例
```bash
curl http://你的服务器IP:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct",
    "messages": [
      {"role": "user", "content": "写一个快速排序"}
    ],
    "temperature": 0.6,
    "top_p": 0.85,
    "max_tokens": 2048
  }'
```

## 🔧 集成到现有项目

### 后端集成（Express）
1. 创建服务模块：`server/services/iquestCoderService.js`
2. 添加 API 路由：`/api/iquest-coder/*`
3. 配置环境变量：`IQUEST_CODER_API_BASE`

### 前端集成（React）
1. 创建 API 服务：`src/services/iquestCoderApi.ts`
2. 创建组件：`src/components/CodeAssistant.tsx`
3. 调用 API 接口

详见：`集成到现有项目.md`

## 📈 监控和维护

### 日志查看
```bash
# Docker 方式
docker logs -f iquest-coder-api

# 直接部署方式
pm2 logs iquest-coder
```

### 性能监控
```bash
# GPU 监控
nvidia-smi -l 1

# 容器监控
docker stats iquest-coder-api

# 进程监控
pm2 monit
```

### 重启服务
```bash
# Docker 方式
docker-compose restart

# 直接部署方式
pm2 restart iquest-coder
```

## 🛡️ 安全建议

1. **配置防火墙**
   ```bash
   sudo ufw allow 8000/tcp
   ```

2. **使用 Nginx 反向代理**
   - 添加 SSL 证书
   - 配置访问控制
   - 设置速率限制

3. **API 密钥验证**（可选）
   - 在 Nginx 层添加认证
   - 或在应用层实现 API key 验证

4. **限制访问 IP**
   - 只允许特定 IP 访问
   - 使用腾讯云安全组

## 🐛 常见问题解决

### 1. 显存不足 (OOM)
```bash
# 减小上下文长度
MAX_MODEL_LEN=16384

# 减小显存利用率
GPU_MEMORY_UTILIZATION=0.85

# 减小批处理大小
MAX_NUM_BATCHED_TOKENS=4096
```

### 2. 模型下载慢
```bash
# 使用 HuggingFace 镜像
export HF_ENDPOINT=https://hf-mirror.com
```

### 3. 推理速度慢
- 检查 GPU 利用率：`nvidia-smi`
- 确认量化已启用
- 调整批处理大小

### 4. 端口被占用
```bash
# 查看端口占用
sudo lsof -i :8000

# 修改端口
PORT=8001
```

## 📞 技术支持

### 官方资源
- [IQuest-Coder 官网](https://iquestcoder.ai/)
- [GitHub 仓库](https://github.com/IQuestLab/IQuest-Coder-V1)
- [HuggingFace 模型页](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct)
- [vLLM 文档](https://docs.vllm.ai/)

### 社区支持
- GitHub Issues
- HuggingFace Discussions
- vLLM Discord

## ✅ 下一步行动

1. **立即部署**
   ```bash
   # 上传文件到服务器
   scp -r iquest-coder-deploy ubuntu@你的服务器IP:~/
   
   # SSH 连接
   ssh ubuntu@你的服务器IP
   
   # 进入目录
   cd iquest-coder-deploy
   
   # 启动服务（Docker 方式）
   docker-compose up -d
   
   # 或启动服务（直接部署方式）
   sudo bash install.sh
   bash start_service.sh
   ```

2. **测试 API**
   ```bash
   python3 test_api.py
   ```

3. **集成到项目**
   - 参考 `集成到现有项目.md`
   - 创建服务模块
   - 添加 API 路由
   - 前端调用

4. **配置外网访问**
   - 开放防火墙端口
   - 配置腾讯云安全组
   - 设置 Nginx 反向代理（可选）

5. **监控和优化**
   - 查看日志
   - 监控性能
   - 根据实际情况调整参数

## 🎉 总结

你现在拥有：
- ✅ 完整的部署方案（Docker + 直接部署）
- ✅ 详细的配置文档
- ✅ API 测试脚本
- ✅ 客户端使用示例
- ✅ 项目集成指南
- ✅ 故障排查方案

**预计部署时间**：
- Docker 方式：3-5 分钟（不含模型下载）
- 直接部署方式：10-15 分钟（不含模型下载）
- 首次模型下载：10-20 分钟（取决于网速）

**开始部署吧！** 🚀

如有问题，请参考：
- `README.md` - 完整文档
- `QUICK_START.md` - 快速开始
- `集成到现有项目.md` - 集成指南
