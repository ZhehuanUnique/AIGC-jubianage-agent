# IQuest-Coder-V1 Docker 部署（国内优化版本）
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# 设置环境变量
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# 使用国内镜像源
RUN sed -i 's@//.*archive.ubuntu.com@//mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list && \
    sed -i 's@//.*security.ubuntu.com@//mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    wget \
    curl \
    vim \
    && rm -rf /var/lib/apt/lists/*

# 配置 pip 国内镜像
RUN pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && \
    pip3 config set install.trusted-host pypi.tuna.tsinghua.edu.cn

# 升级 pip
RUN pip3 install --no-cache-dir --upgrade pip

# 安装 PyTorch (CUDA 12.1) - 使用清华镜像
RUN pip3 install --no-cache-dir \
    torch==2.5.1 \
    torchvision==0.20.1 \
    torchaudio==2.5.1 \
    -i https://pypi.tuna.tsinghua.edu.cn/simple

# 安装 vLLM 和相关依赖
RUN pip3 install --no-cache-dir \
    vllm==0.6.5 \
    transformers==4.56.0 \
    accelerate==0.34.2 \
    bitsandbytes==0.44.1 \
    sentencepiece \
    protobuf \
    -i https://pypi.tuna.tsinghua.edu.cn/simple

# 安装 Flash Attention（可选，提升性能）
RUN pip3 install --no-cache-dir \
    flash-attn==2.7.3 --no-build-isolation \
    -i https://pypi.tuna.tsinghua.edu.cn/simple || echo "Flash Attention 安装失败，跳过"

# 配置 HuggingFace 国内镜像
ENV HF_ENDPOINT=https://hf-mirror.com

# 创建工作目录
WORKDIR /app

# 复制启动脚本
COPY start_vllm_optimized.sh /app/start_vllm.sh
RUN chmod +x /app/start_vllm.sh

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# 启动服务
CMD ["/app/start_vllm.sh"]
