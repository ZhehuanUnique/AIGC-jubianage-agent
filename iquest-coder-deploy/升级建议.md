# 🔧 IQuest-Coder 部署升级建议

## 📊 当前服务器配置分析

根据你的腾讯云服务器配置：
- **GPU**: RTX 5090 (32GB VRAM) ⚠️
- **CPU**: 25 vCPU Intel(R) Xeon(R) Platinum 8470Q
- **内存**: 90GB ⚠️
- **系统盘**: 50GB
- **数据盘**: 0GB（扩容中）
- **PyTorch**: 2.8.0
- **Python**: 3.12
- **CUDA**: 12.8

## 🚨 关键问题

### 1. 显存不足（最严重）

**问题**：
- RTX 5090 只有 32GB 显存
- IQuest-Coder-V1-40B 即使 4-bit 量化也需要 20-22GB
- 推理时还需要额外显存用于 KV Cache
- **实际可用显存可能不足以稳定运行 40B 模型**

**影响**：
- ❌ 可能频繁 OOM（显存溢出）
- ❌ 无法处理长上下文
- ❌ 并发能力极低
- ❌ 推理速度慢

**解决方案**：

#### 方案 A: 使用 14B 模型（强烈推荐）✅
```bash
MODEL_NAME="IQuestLab/IQuest-Coder-V1-14B-Instruct"
# 显存需求：8-10GB（4-bit 量化）
# 性能：仍然非常强大，适合大多数场景
# 优势：稳定、快速、支持长上下文
```

**14B vs 40B 对比**：
| 指标 | 14B | 40B |
|------|-----|-----|
| 显存需求 | 8-10GB | 20-22GB |
| 推理速度 | 快（40-60 tokens/s） | 慢（20-30 tokens/s） |
| 上下文长度 | 支持 32K+ | 受限于显存 |
| 并发能力 | 高（可同时处理多个请求） | 低 |
| 稳定性 | 高 | 中等（容易 OOM） |
| 代码质量 | 优秀 | 更优秀 |

#### 方案 B: 使用 7B 模型（备选）
```bash
MODEL_NAME="IQuestLab/IQuest-Coder-V1-7B-Instruct"
# 显存需求：4-6GB
# 适合：高并发场景、快速响应
```

#### 方案 C: 升级硬件（长期方案）
- 升级到 **RTX 6000 Ada (48GB)** 或 **A100 (40GB/80GB)**
- 或使用 **多卡并行**（需要 2-4 张 RTX 5090）

### 2. 内存不足

**问题**：
- 90GB 内存对于 40B 模型略显不足
- 模型加载、推理、缓存都需要内存

**建议**：
- 升级到 **128GB 或更多**
- 或使用 14B 模型（内存需求更低）

### 3. 系统盘空间不足

**问题**：
- 系统盘只有 50GB
- 模型文件约 20-40GB
- Docker 镜像、日志等也需要空间

**建议**：
- 扩容系统盘到 **100GB+**
- 或将模型缓存到数据盘

### 4. 软件版本优化

**当前版本**：
- PyTorch 2.8.0（太新，可能不稳定）
- Python 3.12（部分库可能不兼容）

**推荐版本**：
```dockerfile
# Python 3.10（最稳定）
python3.10

# PyTorch 2.5.1（稳定版本）
torch==2.5.1

# vLLM 0.6.5（最新稳定版）
vllm==0.6.5
```

## 🎯 推荐配置方案

### 方案 1: 稳定高性能（推荐）✅

```bash
# 模型选择
MODEL_NAME="IQuestLab/IQuest-Coder-V1-14B-Instruct"

# 显存配置
GPU_MEMORY_UTILIZATION=0.85
MAX_MODEL_LEN=32768  # 支持 32K 上下文

# 性能配置
MAX_NUM_BATCHED_TOKENS=8192
MAX_NUM_SEQS=256

# 量化
QUANTIZATION="bitsandbytes"  # 4-bit
```

**优势**：
- ✅ 显存充足（使用约 10GB）
- ✅ 推理速度快
- ✅ 支持长上下文
- ✅ 高并发能力
- ✅ 稳定可靠

### 方案 2: 极致性能（冒险）⚠️

```bash
# 模型选择
MODEL_NAME="IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct"

# 显存配置（激进）
GPU_MEMORY_UTILIZATION=0.95
MAX_MODEL_LEN=8192  # 只支持 8K 上下文

# 性能配置（保守）
MAX_NUM_BATCHED_TOKENS=2048
MAX_NUM_SEQS=32

# 量化（更激进）
QUANTIZATION="gptq"  # 3-bit（如果支持）
```

**风险**：
- ⚠️ 可能频繁 OOM
- ⚠️ 上下文长度受限
- ⚠️ 并发能力低
- ⚠️ 推理速度慢

### 方案 3: 高并发（适合生产）

```bash
# 模型选择
MODEL_NAME="IQuestLab/IQuest-Coder-V1-7B-Instruct"

# 显存配置
GPU_MEMORY_UTILIZATION=0.80
MAX_MODEL_LEN=32768

# 性能配置（高并发）
MAX_NUM_BATCHED_TOKENS=16384
MAX_NUM_SEQS=512

# 量化
QUANTIZATION="bitsandbytes"
```

**优势**：
- ✅ 极快的响应速度
- ✅ 支持大量并发
- ✅ 显存占用低
- ✅ 适合生产环境

## 📦 升级文件

我已经为你创建了优化版本：

1. **Dockerfile.国内镜像** - 使用国内镜像加速
2. **start_vllm_optimized.sh** - 优化的启动脚本（默认使用 14B）

## 🚀 立即升级步骤

### 步骤 1: 使用优化版本部署

```bash
# 使用国内镜像版本
docker build -t iquest-coder-api -f Dockerfile.国内镜像 .

# 启动服务（默认使用 14B 模型）
docker run -d \
  --name iquest-coder \
  --gpus all \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  iquest-coder-api
```

### 步骤 2: 监控显存使用

```bash
# 实时监控 GPU
watch -n 1 nvidia-smi

# 查看日志
docker logs -f iquest-coder
```

### 步骤 3: 性能测试

```bash
# 运行测试脚本
python3 test_api.py

# 压力测试（可选）
ab -n 100 -c 10 -p test_payload.json -T application/json \
  http://localhost:8000/v1/chat/completions
```

## 🔍 性能对比

### 14B 模型（推荐）

| 指标 | 数值 |
|------|------|
| 显存占用 | 8-10GB |
| 推理速度 | 40-60 tokens/s |
| 最大上下文 | 32K tokens |
| 并发能力 | 高（256 并发） |
| 稳定性 | 优秀 |
| 代码质量 | 优秀 |

### 40B 模型（冒险）

| 指标 | 数值 |
|------|------|
| 显存占用 | 20-22GB |
| 推理速度 | 20-30 tokens/s |
| 最大上下文 | 8-16K tokens |
| 并发能力 | 低（32 并发） |
| 稳定性 | 中等（易 OOM） |
| 代码质量 | 更优秀 |

## 💡 其他优化建议

### 1. 启用 Flash Attention 2
```bash
pip install flash-attn==2.7.3 --no-build-isolation
```

### 2. 使用 HuggingFace 国内镜像
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

### 3. 配置 Nginx 缓存
```nginx
proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=api_cache:10m max_size=1g;

location /v1/chat/completions {
    proxy_cache api_cache;
    proxy_cache_valid 200 1h;
    proxy_cache_key "$request_uri|$request_body";
}
```

### 4. 使用 Redis 缓存结果
```javascript
// 缓存常见问题的答案
const redis = require('redis')
const client = redis.createClient()

async function getCachedResponse(prompt) {
  const cached = await client.get(prompt)
  if (cached) return JSON.parse(cached)
  
  const response = await generateCode(prompt)
  await client.setex(prompt, 3600, JSON.stringify(response))
  return response
}
```

## ✅ 最终建议

**强烈推荐使用 14B 模型**，原因：

1. ✅ **显存充足** - 只用 10GB，留有大量余量
2. ✅ **性能优秀** - 代码质量接近 40B
3. ✅ **速度快** - 推理速度是 40B 的 2 倍
4. ✅ **稳定可靠** - 不会 OOM
5. ✅ **支持长上下文** - 可以处理 32K tokens
6. ✅ **高并发** - 可同时处理多个请求

**除非你有以下需求，否则不建议使用 40B**：
- 需要最顶级的代码质量
- 只处理单个请求（不需要并发）
- 可以接受较慢的响应速度
- 愿意升级硬件

## 📞 需要帮助？

如果你决定升级，我可以帮你：
1. 修改配置文件
2. 测试不同模型
3. 优化性能参数
4. 解决部署问题

**现在就开始使用 14B 模型吧！** 🚀
